#!/usr/bin/env python

#starting point: train, val, test in memory from data_prep

def dummy_run(spark):
    train=spark.createDataFrame(
    [
        (82, 124, 5.0),
        (64, 123, 4.0),
        (27, 122, 3.0),
        (25, 122, 1.0),
        (12, 124, 2.0)
    ],
    ['user_id', 'book_id', 'rating'] 
    )

    val=spark.createDataFrame(
    [
        (82, 123, 5.0),
        (64, 122, 4.0),
        (27, 124, 3.0),
        (64, 123, 2.0),
        (12, 122, 4.0)
    ],
    ['user_id', 'book_id', 'rating'] 
    )
    
    predictions=als(spark, train, val, lamb=0.01, rank=3)
    print(predictions)
    print(type(predictions))
    predictions.show()
    return 

def als(spark, train, val, lamb, rank):
    ''' 
        Fits ALS model from train and makes predictions 
        Imput: training file
        arguments:
            spark - spark
            lamba - 
            rank - 
        Returns: Predictions generated by als 
    Notes: 
        https://spark.apache.org/docs/2.2.0/ml-collaborative-filtering.html
        - Don't need to consider alpha bc using explicit feedback
        - Assignment readme : only need to tune rank and lambda...will leave other als params 
        - Question: not sure what to do about the nonnegative param in als "specifies whether or not to
         use nonnegative constraints for least squares (defaults to false)"
        - "Currently the supported cold start strategies are 'nan' and 'drop'. Spark allows users to set the 
        coldStartStrategy parameter to “drop” in order to drop any rows in the DataFrame of predictions that contain NaN values. 
        The evaluation metric will then be computed over the non-NaN data and will be valid" 
       
    '''
    from pyspark.ml.recommendation import ALS

    als = ALS(rank = rank, regParam=lamb, userCol="user_id", itemCol="book_id", ratingCol='rating', implicitPrefs=False, coldStartStrategy="drop")
    model = als.fit(train)
   
    predictions = model.transform(val)
    return predictions

def evaluate(truth, preds, k):

    #reference: https://vinta.ws/code/spark-ml-cookbook-pyspark.html

    from pyspark.ml.evaluation import RegressionEvaluator
    from pyspark.mllib.evaluation import RankingMetrics
    from pyspark.sql import Window
    from pyspark.sql.functions import col, expr
    import pyspark.sql.functions as F
    
    #make the true utility matrix
    true_value = val.select('user_id', 'book_id').groupBy('user_id').agg(expr('collect_list(track_id_indexed) as true_value'))

    windowSpec = Window.partitionBy('user').orderBy(col('prediction').desc())
    perUserPredictedItemsDF = outputDF \
        .select('user', 'item', 'prediction', F.rank().over(windowSpec).alias('rank')) \
        .where('rank <= {0}'.format(k)) \
        .groupBy('user') \
        .agg(expr('collect_list(item) as items'))
    perUserPredictedItemsDF.show()

    windowSpec = Window.partitionBy('from_user_id').orderBy(col('starred_at').desc())
    perUserActualItemsDF = rawDF \
        .select('from_user_id', 'repo_id', 'starred_at', F.rank().over(windowSpec).alias('rank')) \
        .where('rank <= {0}'.format(k)) \
        .groupBy('from_user_id') \
        .agg(expr('collect_list(repo_id) as items')) \
        .withColumnRenamed('from_user_id', 'user')

    perUserItemsRDD = perUserPredictedItemsDF.join(perUserActualItemsDF, 'user') \
        .rdd \
        .map(lambda row: (row[1], row[2]))
    rankingMetrics = RankingMetrics(perUserItemsRDD)

    print("Precision at k: ", rankingMetrics.precisionAt(k))
    print("MAP : ", rankingMetrics.meanAveragePrecision)
    print("NDCG at k: ", rankingMetrics.ndcgAt(k))

#use spark grid search -- see example code
def hyperparam_search(train, val):

    lambs=[0.0001, 0.001, 0.01, 0.1, 1, 10]
    ranks=[5, 10, 100, 500, 1000, 10000]

    for i in lambs:
        for j in ranks:
            model=fit_als(train, lamb=i, rank=j)


     # Tune hyper-parameters with cross-validation 
     # reference: 
    paramGrid = ParamGridBuilder() \
        .addGrid(lr.regParam, [0.0001, 0.001, 0.01, 0.1, 1]) \
        .addGrid(lr.elasticNetParam, [0, 0.25, 0.5, 0.75, 1]) \
        .build()

    crossval = CrossValidator(estimator=lr,
                              estimatorParamMaps=paramGrid,
                              evaluator=MulticlassClassificationEvaluator(),
                              numFolds=5)

    # Build the pipeline
    pipeline = Pipeline(stages=[assembler, scaler, indexer, crossval])
    
    # Train the model
    pipelineModel = pipeline.fit(data)

    # Save the model
    pipelineModel.save(model_file)

    # Get best LR hyper-parameters
    model = pipelineModel.stages[-1].bestModel
    print('Best Param (regParam): ', model.getOrDefault('regParam'))
    print('Best Param (elasticNetParam): ', model.getOrDefault('elasticNetParam'))





